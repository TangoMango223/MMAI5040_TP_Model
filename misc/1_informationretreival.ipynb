{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86faad-0322-4638-9f7c-a60e97cdaba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-24331f781ace>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully crawled: https://www.tps.ca/crime-prevention/\n",
      "Successfully crawled: https://www.toronto.ca/community-people/public-safety-alerts/community-safety-programs/\n",
      "Rate limit hit. Waiting 34 seconds before retry 1/3\n",
      "Error details: Unexpected error during start crawl job: Status code 429. Rate limit exceeded. Consumed (req/min): 3, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 34s, resets at Sat Nov 09 2024 22:29:43 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "Successfully crawled: https://www.ttc.ca/riding-the-ttc/safety-and-security/Travelling-Safely/TTC-Safety-Guide\n",
      "Successfully crawled: https://www.toronto.ca/community-people/public-safety-alerts/\n",
      "Rate limit hit. Waiting 22 seconds before retry 1/3\n",
      "Error details: Unexpected error during start crawl job: Status code 429. Rate limit exceeded. Consumed (req/min): 3, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 22s, resets at Sat Nov 09 2024 22:30:49 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "Successfully crawled: https://www.toronto.ca/community-people/public-safety-alerts/safety-tips-prevention/\n",
      "Successfully crawled: https://www.getprepared.gc.ca/cnt/rsrcs/sfttps/index-en.aspx\n",
      "Rate limit hit. Waiting 36 seconds before retry 1/3\n",
      "Error details: Unexpected error during start crawl job: Status code 429. Rate limit exceeded. Consumed (req/min): 3, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 36s, resets at Sat Nov 09 2024 22:31:54 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "Successfully crawled: https://www.tps.ca/services/\n",
      "Successfully crawled: https://www.tps.ca/organizational-chart/communities-neighbourhoods-command/field-services/community-partnerships-engagement-unit/\n",
      "Rate limit hit. Waiting 20 seconds before retry 1/3\n",
      "Error details: Unexpected error during start crawl job: Status code 429. Rate limit exceeded. Consumed (req/min): 3, Remaining (req/min): 0. Upgrade your plan at https://firecrawl.dev/pricing for increased rate limits or please retry after 20s, resets at Sat Nov 09 2024 22:32:58 GMT+0000 (Coordinated Universal Time) - No additional error details provided.\n",
      "Successfully crawled: https://www.retailcouncil.org/wp-content/uploads/2024/10/Retail-Safety-and-Security-Guide-1.pdf\n",
      "Successfully crawled: https://www.tps.ca/chief/chiefs-office/missing-and-missed-implementation/missing-person-awareness-day/\n",
      "Data has been written to json file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "scrape_torontopolice.py\n",
    "Goal: Scraping from Crime Prevention Articles (and other sources), using FireCrawl.\n",
    "\"\"\"\n",
    "\n",
    "# ------ Package installation ------\n",
    "\n",
    "# Packages to handle pip installs\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Import Statements for writing/reading with Google Sheets API\n",
    "import requests\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from urllib.parse import urlparse, quote\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import PyPDF2\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# FireCrawl App:\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Variables:\n",
    "JSON_FILE = \"torontopublicsafetycorpus.json\"\n",
    "crawl_results = [] # store in json format\n",
    "\n",
    "\n",
    "# Some keys and JSON files for API Keys are hidden in this repo.\n",
    "\n",
    "\n",
    "# ------ Part 1: Check Package Requirements ------\n",
    "\n",
    "def check_and_install_requirements():\n",
    "    requirements_path = 'requirements.txt'\n",
    "    with open(requirements_path, 'r') as f:\n",
    "        required_packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
    "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    missing = set(required_packages) - installed\n",
    "    if missing:\n",
    "        print(f\"Installing missing packages: {', '.join(missing)}\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', requirements_path], stdout=subprocess.DEVNULL)\n",
    "        print(\"Installation complete.\")\n",
    "\n",
    "#check_and_install_requirements()\n",
    "\n",
    "\n",
    "# ------ Connect to Google Sheets again ------\n",
    "\n",
    "# Configuration\n",
    "SPREADSHEET_ID = '1PEVuqlUrvVoJJUflYGq4gN6itj9WZzGB-UHYnsIusng'  # Updated with correct ID\n",
    "RANGE_NAME = 'Sheet1!C:C'\n",
    "SERVICE_ACCOUNT_FILE = 'torontopolicellmbot-9644404c0bdf.json'\n",
    "\n",
    "# Initialize Google Sheets API\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, \n",
    "    scopes=['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    ")\n",
    "service = build('sheets', 'v4', credentials=credentials)\n",
    "\n",
    "def get_urls_from_sheet():\n",
    "    try:\n",
    "        # Direct API call without using sheet variable\n",
    "        result = service.spreadsheets().values().get(\n",
    "            spreadsheetId=SPREADSHEET_ID,\n",
    "            range=RANGE_NAME\n",
    "        ).execute()\n",
    "        values = result.get('values', [])\n",
    "        \n",
    "        if not values:\n",
    "            # print('No data found in the sheet.')\n",
    "            return []\n",
    "            \n",
    "        # Filter for valid URLs and remove empty rows\n",
    "        urls = [row[0] for row in values if row and row[0].startswith('http')]\n",
    "        # print(f\"Found {len(urls)} valid URLs in the sheet\")\n",
    "        return urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URLs from Google Sheets: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the connection\n",
    "resources = get_urls_from_sheet()\n",
    "\n",
    "# ------ Part 2: Firecrawl Websites ------\n",
    "# Initialize app\n",
    "app = FirecrawlApp(api_key= os.environ.get(\"FIRECRAWL_KEY\"))\n",
    "\n",
    "# Crawl Website:\n",
    "crawl_params = {\n",
    "    'limit': 200,  # Limit to 200 pages\n",
    "    'depth': 2,  # Crawl up to 2 levels deep from the start page\n",
    "    'scrapeOptions': {\n",
    "        'formats': ['html', 'markdown'],  # Save content in HTML and Markdown formats\n",
    "        'customSelectors': {  # Specific elements to extract\n",
    "            'titles': 'h1, h2',  # Extract main headings for structure\n",
    "            'paragraphs': 'p',   # Extract paragraphs for main content\n",
    "        }\n",
    "    },\n",
    "    'crawlDelay': 2,  # 2-second delay between requests\n",
    "    'userAgent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',  # Mimic a common browser user agent\n",
    "}\n",
    "\n",
    "def extract_wait_time(error_message):\n",
    "    \"\"\"Extract wait time from FireCrawl error message\"\"\"\n",
    "    wait_time_match = re.search(r'retry after (\\d+)s', str(error_message))\n",
    "    if wait_time_match:\n",
    "        return int(wait_time_match.group(1))\n",
    "    return 60  # default wait time if we can't parse the message\n",
    "\n",
    "def crawl_with_retry(app, link, max_retries=3):\n",
    "    \"\"\"\n",
    "    Attempt to crawl a URL with retry logic for rate limiting\n",
    "    Uses the exact wait time specified in the error message\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = app.crawl_url(link)\n",
    "            print(f\"Successfully crawled: {link}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):  # Rate limit exceeded\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = extract_wait_time(str(e))\n",
    "                    print(f\"Rate limit hit. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n",
    "                    print(f\"Error details: {str(e)}\")\n",
    "                    time.sleep(wait_time + 5)  # Add 5 seconds buffer to be safe\n",
    "                else:\n",
    "                    print(f\"Failed to crawl {link} after {max_retries} attempts\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Error crawling {link}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "# Replace the crawling loop with this:\n",
    "for link in resources:\n",
    "    result = crawl_with_retry(app, link)\n",
    "    if result is not None:\n",
    "        crawl_results.append(result)\n",
    "    time.sleep(5)  # Add a small delay between successful crawls\n",
    "\n",
    "# ------ Part 3: Write to JSON file  ------\n",
    "# Save the crawl results to a JSON file\n",
    "with open(JSON_FILE, 'w') as f:\n",
    "    # We will re-write it each time.\n",
    "    json.dump(crawl_results, f, indent=2)\n",
    "\n",
    "print(\"Data has been written to json file.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
